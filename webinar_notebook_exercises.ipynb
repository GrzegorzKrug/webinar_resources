{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYQ6Lh11LFkP"
   },
   "source": [
    "# WEBINAR - Przetwarzanie języka naturalnego, 16 lipca 2020\n",
    "\n",
    "SAGES - NLP masterclass\n",
    "\n",
    "Łukasz Kobyliński, Ryszard Tuora\n",
    "\n",
    "\n",
    "2 modele do języka polskiego w spaCy:\n",
    "\n",
    "IPI PAN - bardziej rozbudowany, wolniejszy, bardziej złożona instalacja - http://zil.ipipan.waw.pl/SpacyPL\n",
    "\n",
    "model oficjalny - prostszy, znacznie szybszy, prosta instalacja - https://spacy.io/models/pl\n",
    "\n",
    "model IPI PAN dla języka polskiego\n",
    "\n",
    "składa się z:\n",
    "- taggera morfosyntaktycznego\n",
    "- lematyzatora\n",
    "- parsera zależnościowego\n",
    "- komponentu NER\n",
    "- flexera (komponentu do fleksji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RprQFxeM4zas"
   },
   "outputs": [],
   "source": [
    "# Przygotowanie środowiska, komendy linux\n",
    "# instalacja Morfeusza 2\n",
    "!wget -O - http://download.sgjp.pl/apt/sgjp.gpg.key|sudo apt-key add -\n",
    "!sudo apt-add-repository http://download.sgjp.pl/apt/ubuntu\n",
    "!sudo apt update\n",
    "!sudo apt install morfeusz2\n",
    "!sudo apt install python3-morfeusz2\n",
    "\n",
    "# upgrade keras'a\n",
    "!python3 -m pip install --upgrade keras==2.3.1\n",
    "\n",
    "# instalacja spaCy\n",
    "\n",
    "!python3 -m pip install spacy==2.3.0\n",
    "\n",
    "# 1. instalacja modelu IPI PAN dla języka polskiego\n",
    "!wget \"http://zil.ipipan.waw.pl/SpacyPL?action=AttachFile&do=get&target=pl_spacy_model_morfeusz-0.1.0.tar.gz\"\n",
    "!mv 'SpacyPL?action=AttachFile&do=get&target=pl_spacy_model_morfeusz-0.1.0.tar.gz' pl_spacy_model_morfeusz-0.1.0.tar.gz\n",
    "!python3 -m pip install pl_spacy_model_morfeusz-0.1.0.tar.gz\n",
    "\n",
    "# linkowanie modelu do spaCy\n",
    "!python3 -m spacy link pl_spacy_model_morfeusz pl_spacy_model_morfeusz -f\n",
    "\n",
    "# 2. instalacja oficjalnego modelu spaCy\n",
    "!python3 -m spacy download pl_core_news_lg\n",
    "\n",
    "# dodatkowe zależności:\n",
    "!python3 -m pip install tqdm\n",
    "\n",
    "# PO WYKONANIU NALEŻY ZRESETOWAĆ RUNTIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFt4fu4nXrRd"
   },
   "source": [
    "**Po wykonaniu powyższej komórki należy zresetować runtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PtEk9Bt0N3u1"
   },
   "outputs": [],
   "source": [
    "### PYTHON 3\n",
    "import keras\n",
    "print(keras.__version__) # Powinno wyświetlić 2.3.1\n",
    "# ładowanie modelu\n",
    "import spacy\n",
    "print(spacy.__version__)\n",
    "\n",
    "nlp = spacy.load(\"pl_spacy_model_morfeusz\")\n",
    "#nlp = spacy.load(\"pl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zquQAX2HGVpg"
   },
   "source": [
    "# Część zero - Tokenizacja i reprezentacja tekstów\n",
    "\n",
    "Wejściem do modelu są łańcuchy tekstowe (stringi), na wyjściu dostajemy obiekt reprezentujący struktury wykryte w tekście. Pierwszym krokiem który musi być wykonany aby przetworztć tekst, jest tokenizacja, czyli podział tekstu na tokeny/segmenty. Tokeny w większości przypadków odpowiadają słowom \"od spacji do spacji\". Ale warto zwrócić uwagę na kilka przypadków odstających od takiej prostej reguły.\n",
    "\n",
    "Wynikiem potoku spaCy jest obiekt Doc, który składa się z obiektów Token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OrMbeDcfHIvu"
   },
   "outputs": [],
   "source": [
    "txt = \"Chciałby, żebym pojechał do miasta z zielono-żółto-białą flagą (np. Zielonej Góry).\"\n",
    "split = txt.split()\n",
    "doc = nlp(txt)\n",
    "print(\"spaCy          .split()\\n\")\n",
    "for i in range(max([len(split), len(doc)])):\n",
    "  try:\n",
    "    tok1 = doc[i]\n",
    "  except IndexError:\n",
    "    tok1 = \"\"\n",
    "  try:\n",
    "    tok2 = split[i]\n",
    "  except IndexError:\n",
    "    tok2 = \"\"\n",
    "  print(\"{0:15} {1:15}\".format(tok1.orth_, tok2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuPWYncPLyK4"
   },
   "source": [
    "# Część pierwsza - Tagowanie morfosyntaktyczne\n",
    "korzystamy z tagsetu NKJP\n",
    "Nasz tagger to słownikowy tagger Morfeusz2 + dezambiguacja za pomocą neuronowego Toyggera (LSTM)\n",
    "\n",
    "Każdy token t ma trzy interesujące nas atrybuty: \n",
    "- t.tag_ : klasa gramatyczna według polskiego tagsetu NKJP (http://nkjp.pl/poliqarp/help/ense2.html)\n",
    "- t.pos_ : klasa gramatyczna według międzynarodowego tagsetu UD (mapowana z NKJP)\n",
    "- t._.feats : customowy atrybut odpowiadający cechom morfosyntaktycznym (np. rodzajowi gramatycznemu, lub liczbie), poszczególne wartości cech są oddzielone dwukropkiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8MPXgjqU2LH"
   },
   "outputs": [],
   "source": [
    "txt = \"Nornica prowadzi zmierzchowo-nocny tryb życia, ale wychodzi również za dnia w poszukiwaniu pokarmu.\"\n",
    "doc = nlp(txt) # przetworzenie textu przez pipeline, na wyjściu dostajemy iterowalny obiekty klasy Doc, przechowujący tokeny\n",
    "for t in doc:\n",
    "  print(\"{0:15} {1:8} {2:6} {3:15}\".format(t.orth_, t.tag_, t.pos_, t._.feats)) # wypisujemy interpretację morfosyntaktyczną każdego tokenu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZ2ioo6nW8eo"
   },
   "source": [
    "##Zadanie 1:\n",
    "\n",
    "\n",
    "![alt text](https://github.com/ryszardtuora/webinar_resources/raw/master/czesci_mowy.png)\n",
    "\n",
    "Źródło: Irena Kamińska-Szmaj, *Różnice leksykalne między stylami funkcjonalnymi polszczyzny pisanej: Analiza statystyczna na materiale słownika frekwencyjnego*, 1990.\n",
    "\n",
    "Przetwórz tekst znajdujący się w zmiennej txt, oblicz proporcję czasowników (używając tagów UPOS), na podstawie tego oszacuj gatunek do którego należy tekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2eBs1u9VoMR"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "txt = requests.get(\"https://raw.githubusercontent.com/ryszardtuora/webinar_resources/master/1.txt\").text\n",
    "\n",
    "#TODO#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzHlNwhrthsl"
   },
   "source": [
    "### Fleksja\n",
    "\n",
    "Flexer pozwala na odmianę pojedynczych tokenów, do pożądanej charakterystyki morfologicznej. Argumentami do flexera jest słowo do odmiany (string, lub lepiej otagowany token), i przedzielony dwukropkami string znaczników morfosyntaktycznych.\n",
    "\n",
    "Flexer umożliwia np. wypełnianie szablonów tekstów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4bvg3thmuzt"
   },
   "outputs": [],
   "source": [
    "filizanka = nlp(\"filiżanka\")[0]\n",
    "print(filizanka._.feats)\n",
    "flexer = nlp.get_pipe(\"flexer\")\n",
    "\n",
    "tmpl1 = \"Szukam szarej, porcelanowej {}.\".format(flexer.flex(filizanka, \"gen\"))\n",
    "tmpl2 = \"Marzę o szarej, porcelanowej {}.\".format(flexer.flex(filizanka, \"loc\"))\n",
    "tmpl3 = \"Szukam kompletu porcelanowych {}.\".format(flexer.flex(filizanka, \"gen:pl\"))\n",
    "print(tmpl1)\n",
    "print(tmpl2)\n",
    "print(tmpl3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "noq6HpCgY4q2"
   },
   "source": [
    "#Część druga - Lematyzacja i własności leksykalne\n",
    "Nasz model umożliwia słownikową lematyzację przy pomocy Morfeusza, do dezambiguacji (tutaj np. rozróżnienia między \"mamy\"-> \"mama\" i \"mamy\" -> \"mieć\" służy output taggera).\n",
    "\n",
    "Lematyzacja pozwala redukować redundancję informacyjną, i ułatwiać zadania takie jak streszczanie, i przeszukiwanie.\n",
    "\n",
    "Każdy z tokenów dodatkowo jest oznaczony ze względu na pewne własności leksykalne, np. :\n",
    "- t.is_stop - słowo należy do stoplisty (listy słów występujących najczęściej, a więc najmniej istotnych semantycznie)\n",
    "- t.is_oov - słowo znajduje się poza słownikiem, i.e. embeddingami wykorzystanymi w modelu\n",
    "- t.like_url - token ma strukturę url-a\n",
    "- t.like_num - token jest liczbą\n",
    "- t.is_alpha - token składa się tylko ze znaków alfabetycznych\n",
    "- t.rank - miejse w rankingu częstości słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFmVQkr2ZsKu"
   },
   "outputs": [],
   "source": [
    "txt = \"Zdenerwowany gen. Leese mówił przez telefon swym podwładnym walczącym pod Monte Cassino, że rozmawia z nimi ze schronu.\"\n",
    "doc = nlp(txt)\n",
    "print(\"{0:16} {1:16} {2:5} {3:5} {4:20}\\n\".format(\"forma\", \"lemat\", \"OOV\", \"STOP\", \"Częstość\"))\n",
    "for t in doc:\n",
    "  print(\"{0:16} {1:16} {2:5} {3:5} {4:20}\".format(t.orth_, t.lemma_, t.is_oov, t.is_stop, t.rank)) # orth_ to atrybut odpowiadający formie słowa występującej w tekście"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2cBJlSpcziu"
   },
   "source": [
    "##Zadanie 2:\n",
    "\n",
    "Przetwórz tekst spod zmiennej txt, przekonwertuj go do listy lematów, usuń słowa ze stoplisty, oraz interpunkcję, wypisz dziesięć najczęściej pojawiających się lematów. Wypróbuj także opcję w której uwzględniamy tylko rzeczowniki\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HlxUtCEqcy3k"
   },
   "outputs": [],
   "source": [
    "txt = requests.get(\"https://raw.githubusercontent.com/ryszardtuora/webinar_resources/master/2.txt\").text\n",
    "\n",
    "#TODO#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yev0e8lQmwMm"
   },
   "source": [
    "#Część trzecia - parsowanie zależnościowe\n",
    "\n",
    "spaCy zawiera parser zależnościowy oparty o metodologię transition-based dependency parsing. \n",
    "Interesują nas tu cztery atrybuty:\n",
    " - t.head - link do tokenu będącego nadrzędnikiem tokenu t\n",
    " - t.dep_ - etykieta opisująca rodzaj zależności\n",
    " - t.subtree - generator opisujący poddrzewo rozpięte przez token t\n",
    " - t.ancestors - generator opisujący wszystkie przechodnie nadrzędniki tokenu t\n",
    "\n",
    "\n",
    "Opis systemu etykiet: https://universaldependencies.org/u/dep/all.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7i9joa5n3ZK"
   },
   "outputs": [],
   "source": [
    "txt = \"Pierwsza wzmianka o Gdańsku pochodzi ze spisanego po łacinie w 999 Żywotu świętego Wojciecha.\"\n",
    "doc = nlp(txt)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "table = []\n",
    "for tok in doc:\n",
    "  tok_dic = {\"form\": tok.orth_, \"label\": tok.dep_, \"head\": tok.head.orth_, \"subtree\": list(tok.subtree), \"ancestors\": list(tok.ancestors)}\n",
    "  table.append(tok_dic)\n",
    "df = pd.DataFrame(table)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6MI0U4Npeqn"
   },
   "source": [
    "### spaCy posiada wbudowaną wizualizację drzew zależnościowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFX-8z48peHV"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JS6zUzTKmN86"
   },
   "source": [
    "###Poniższa funkcja służy łatwej wizualizacji podstawowych własności tokenów z danego tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PDcn4tWTLBwz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def table(doc):\n",
    "  table = []\n",
    "  for tok in doc:\n",
    "    tok_dic = {\"form\": tok.orth_, \"lemma\": tok.lemma_, \"tag\": \":\".join([tok.tag_, tok._.feats]), \"dep_label\": tok.dep_, \"dep_head\": tok.head.orth_}\n",
    "    table.append(tok_dic)\n",
    "  return pd.DataFrame(table)\n",
    "\n",
    "txt = \"Wiadomość jest symboliczna, ale oznacza też początek długotrwałego trendu.\\\n",
    " Dochód na mieszkańca z uwzględnieniem realnej mocy nabywczej walut narodowych \\\n",
    " wyniósł w 2019 r. w Rzeczpospolitej Polskiej 33 891 dolarów, nieco więcej, niż w Portugalii \\\n",
    " (33 665 dolarów). Jednak Fundusz przewiduje, że w tym roku portugalska gospodarka\\\n",
    "  będzie się rozwijać w tempie 1,6 proc. wobec 3,1 proc. w przypadku gospodarki \\\n",
    "  polskiej. Nożyce między oboma krajami będą się więc rozwierać.\"\n",
    "\n",
    "doc = nlp(txt)\n",
    "\n",
    "tab = table(doc)\n",
    "\n",
    "print(tab.to_string()) # prosty hack na wypisanie całej tabeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_EwypobSuHQZ"
   },
   "source": [
    "###Czasami interesują nas większe całości niż pojedyncze tokeny, np. rzeczowniki często łączą się z przymiotnikami w frazy, żeby znajdować takie całości w tekście możemy korzystać z Matchera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rgH3Y2ybhYxp"
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "doc.is_tagged = True # tymczasowy hack, będzie zbędny w kolejnej wersji modelu\n",
    "pattern = [{\"TAG\": \"adj\"}, {\"TAG\": \"subst\"}]\n",
    "matcher.add(\"AdjSubst\", None, pattern) # nazwa, funkcja, wzór\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "  print(doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tev_A5DLp04E"
   },
   "source": [
    "#### Niemniej w języku polskim, ze względu na swobodę szyku, takie rozwiązanie jest mało owocne, przymiotnik nie musi zawsze poprzedzać rzeczownika który określa, tego typu zależności są widoczne dopiero na poziomie analizy gramatycznej\n",
    "\n",
    "### W gramatykach zależnościowych możliwa jest częściowa rekonstrukcja fraz rzeczownikowych (Noun Phrase - NP)\n",
    "Aby to zrobić, musimy wydobyć z tekstu wszystkie rzeczowniki, zebrać wszystkie rozpinane przez nie poddrzewa, i wyrzucić ze zbioru te poddrzewa, które są już podfrazą większej frazy rzeczownikowej. To frazy rzeczownikowe, a nie same rzeczowniki, są jednostkami które mają dobrze określone znaczenie. Np. w zdaniu:\n",
    "\n",
    "###*Żona Pawła jest blondynką.* \n",
    "\n",
    "nie mówimy o Pawle, ani o jakiejś abstrakcyjnej żonie, tylko o konkretnej kobiecie, którą wyodrębniamy przez fakt że jest żoną Pawła."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKlWTF1aqNgm"
   },
   "source": [
    "##Zadanie 3:\n",
    "\n",
    "Napisz funkcję która na wejściu pobiera dokument, a na wyjściu zwraca listę fraz rzeczownikowych zgodnie z podanym powyżej opisem. Następnie wyodrębnij wszystkie frazy rzeczownikowe z tekstu pod zmienną txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0WkRuafBp0F8"
   },
   "outputs": [],
   "source": [
    "#TODO#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YG0hKgyWwc4N"
   },
   "source": [
    "###Parser zależnościowy pozwala także na dzielenie dokumentów na zdania w sposób bardziej inteligentny, niż posługując się regułami interpunkcji.\n",
    "###Za zdanie uważamy nieprzerwaną sekwencję tokenów które są powiązane relacjami zależnościowymi.\n",
    "###Zdania są zapisane w atrybucie doc.sents dokumentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fH4rg0X4wwIB"
   },
   "outputs": [],
   "source": [
    "for s in doc.sents:\n",
    "  print(s)\n",
    "\n",
    "displacy.render(doc, jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6d5JbrSsnhm"
   },
   "source": [
    "#Część czwarta - Rozpoznawanie jednostek nazewniczych (NER)\n",
    "####Nasz model do spaCy wykorzystuje 6 rodzajów etykiet:\n",
    "- placeName - miejsca antropogeniczne, np. Dania, Londyn\n",
    "- geogName - naturalne miejsca geograficzne, np. Tatry, Kreta\n",
    "- persName - imiona i nazwiska osób, np. J. F. Kennedy, gen. Maczek\n",
    "- orgName - nazwy organizacji, np. NATO, Unia Europejska\n",
    "- date - daty, np. 22 marca 1999, druga połowa kwietnia\n",
    "- time - czas, np. 18:55, pięć po dwunastej\n",
    "\n",
    "####Nie pozwala na wykrywanie zagnieżdżonych jednostek nazewniczych, np. [placeName: **aleja** [persName: **Piłsudskiego**]]\n",
    "\n",
    "####Wykryte wzmianki są przechowywane w atrybucie doc.ents dokumentu, każda z tych wzmianek ma atrybut ent.label_, w którym przechowywana jest jej etykieta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Bkf6kOWt8GK"
   },
   "outputs": [],
   "source": [
    "print(doc, \"\\n\\n\")\n",
    "for e in doc.ents:\n",
    "  print(e, e.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zniWaxZxvYZz"
   },
   "source": [
    "###displaCy pozwala także na wizualizację jednostek nazewniczych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CJOleJmIvfGs"
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cknnUsl1uaKb"
   },
   "source": [
    "W obecnym modelu NER, uwzględnione są \"uniwersalne\" kategorie jednostek nazewniczych, jednak w zależności od zastosowania będziemy najprawdopodobniej potrzebowali innych kategorii - np. osobnej kategorii dla nazw aktów prawodawczych, lub kwot i walut.\n",
    "\n",
    "Bez problemu można zastąpić domyślny model własnym, wytrenowanym przez CLI spaCy na własnych oznakowanych danych. Więcej informacji tu: https://spacy.io/api/cli#train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBxZzfaX28_M"
   },
   "source": [
    "##Zadanie 4:\n",
    "\n",
    "W zmiennej txt znajduje się dokument historyczny, wydobądź z niego wszystkie *zdania* zawierające daty, i po kolei je wypisz, rozważ możliwe sposoby automatycznego szeregowania dat wyrażonych w różny sposób (po ewentualnym sprowadzeniu ich do kanonicznej postaci). Rozwiązanie tego problemu jest trudne, i istnieją do niego odrębne narzędzia, umożliwiają one np. automatyczną rekonstrukcję chronologii wydarzeń."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "glLD6N0V3Uo2"
   },
   "outputs": [],
   "source": [
    "txt = requests.get(\"https://raw.githubusercontent.com/ryszardtuora/webinar_resources/master/3.txt\").text\n",
    "\n",
    "#TODO#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4a5Izt4Tsae"
   },
   "source": [
    "# Część piąta - podobieństwo w języku\n",
    "Większość powyższych metod opiera się o embeddingi, czyli funkcje przypisujące słowom wektory w przestrzeni wielowymiarowej (najczęściej 100 lub 300-wymiarowej). Wektory te nie są przypisywane arbitralnie, lecz własności tej przestrzeni mają w jakiś sposób odzwierciedlać własności języka. Np. odległość między wektorami powinna odpowiadać odległości między słowami, którą możemy rozumieć jako podobieństwo znaczeń."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwvGXnaON7CB"
   },
   "source": [
    "###Bardzo popularny przykład \"arytmetyki słów\":\n",
    "znaczenie słów jest reprezentowane przez wektory, dla których mamy zdefiniowane operacje matematyczne. Możemy więc odjąć od znaczenia słowa \"królowa\", znaczenie słowa \"kobieta\", i dodać doń znaczenie słowa \"mężczyzna\", licząc iż wektor będący wynikiem takiego działania odpowiada słowu \"król\". W praktyce wektor taki najprawdopodobniej nie ma interpretacji, ale możemy znaleźć najbliższy wektor który ma jakąkolwiek interpretację przeszukując słownik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jb9ZXk4a8ety"
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    " \n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    " \n",
    "man = nlp.vocab['mężczyzna'].vector\n",
    "woman = nlp.vocab['kobieta'].vector\n",
    "queen = nlp.vocab['królowa'].vector\n",
    "king = nlp.vocab['król'].vector\n",
    " \n",
    "# We now need to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
    "maybe_king = man - woman + queen\n",
    "computed_similarities = []\n",
    " \n",
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors\n",
    "    if not word.has_vector:\n",
    "        continue\n",
    " \n",
    "    similarity = cosine_similarity(maybe_king, word.vector)\n",
    "    computed_similarities.append((word, similarity))\n",
    " \n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "print([w[0].text for w in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMw2hDepJ9EM"
   },
   "source": [
    "###spaCy umożliwia liczenie podobieństwa między słowami \n",
    "####jest ono proporcjonalne do cosinusa kąta wektorami je reprezentującymi. Odpowiednia funkcja jest zdefiniowana dla leksemów (tutaj oznaczają one słowa z pominięciem kontekstu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXJB6l6IIRJX"
   },
   "outputs": [],
   "source": [
    "w1 = \"pies\"\n",
    "w2 = \"psami\"\n",
    "w3 = \"zwierzę\"\n",
    "w4 = \"buldog\"\n",
    "w5 = \"obroża\"\n",
    "w6 = \"smycz\"\n",
    "w7 = \"marchewka\"\n",
    "w8 = \"słońce\"\n",
    "\n",
    "def similarity(w1, w2):\n",
    "  w1_lex = nlp.vocab[w1]\n",
    "  w2_lex = nlp.vocab[w2]\n",
    "  if w1_lex.has_vector and w2_lex.has_vector:\n",
    "    sim = w1_lex.similarity(w2_lex)\n",
    "    print(\"{} vs. {} -> {}\".format(w1, w2, sim))\n",
    "  else:\n",
    "    print(\"Jedno ze słów nie ma reprezentacji wektorowej.\")\n",
    "for w in [w2, w3, w4, w5, w6, w7, w8]:\n",
    "  similarity(w1, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SKbMLgI7GKd"
   },
   "source": [
    "## Zadanie 5:\n",
    "\n",
    "nlp.vocab może być traktowany jako iterator (stanowi słownik, choć nie wszystkim słowom przypisane są wektory). Napisz funkcję thesaurus(word) która dla podanego słowa znajduje dziesięć najbardziej podobnych słów. Pamiętaj o złożoności obliczeniowej sortowania! \n",
    "\n",
    "Możesz wykorzystać lematyzację, oraz tagowanie morfosyntaktyczne, by pozbyć się zbędnych słów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-i6ko6L67Fpa"
   },
   "outputs": [],
   "source": [
    "#TODO#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "zquQAX2HGVpg",
    "PuPWYncPLyK4",
    "noq6HpCgY4q2",
    "yev0e8lQmwMm",
    "E6d5JbrSsnhm",
    "_4a5Izt4Tsae"
   ],
   "name": "Copy of Copy of Copy of Copy of Copy of Gdańsk.ipynb",
   "provenance": [
    {
     "file_id": "1MWiYw3ugMhpq38tBhkbWi1ZMLHU17IqZ",
     "timestamp": 1594737900299
    },
    {
     "file_id": "1lfYKkZyhW-hfQ6b0qsXxrVTjGquz8Qdn",
     "timestamp": 1594727658531
    },
    {
     "file_id": "1onFhNTAHRwnqjbCsYx1BdgoE76O5Ssix",
     "timestamp": 1594667047412
    },
    {
     "file_id": "1APwoUXjdVCZQVF6B1zMJUy5VZPQU1m43",
     "timestamp": 1594650744668
    },
    {
     "file_id": "https://github.com/ryszardtuora/webinar_resources/blob/master/Gda%C5%84sk.ipynb",
     "timestamp": 1594596301441
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
