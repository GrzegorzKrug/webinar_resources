{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tYQ6Lh11LFkP"
   },
   "source": [
    "# Warsztaty - Przetwarzanie języka naturalnego\n",
    "\n",
    "27 lutego 2020, szkolenie Voicelab\n",
    "\n",
    "Ryszard Tuora\n",
    "\n",
    "model spaCy dla języka polskiego\n",
    "\n",
    "składa się z:\n",
    "- taggera morfosyntaktycznego\n",
    "- lematyzatora\n",
    "- parsera zależnościowego\n",
    "- komponentu NER\n",
    "- flexera (komponentu do fleksji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RprQFxeM4zas"
   },
   "outputs": [],
   "source": [
    "# Przygotowanie środowiska, komendy linux\n",
    "# instalacja Morfeusza 2\n",
    "!wget -O - http://download.sgjp.pl/apt/sgjp.gpg.key|sudo apt-key add -\n",
    "!sudo apt-add-repository http://download.sgjp.pl/apt/ubuntu\n",
    "!sudo apt update\n",
    "!sudo apt install morfeusz2\n",
    "!sudo apt install python3-morfeusz2\n",
    "\n",
    "# upgrade keras'a\n",
    "!python3 -m pip install --upgrade keras\n",
    "\n",
    "# instalacja spaCy\n",
    "\n",
    "!python3 -m pip install spacy\n",
    "\n",
    "# instalacja modelu spaCy dla języka polskiego\n",
    "!wget \"http://zil.ipipan.waw.pl/SpacyPL?action=AttachFile&do=get&target=pl_spacy_model_morfeusz-0.1.0.tar.gz\"\n",
    "!mv 'SpacyPL?action=AttachFile&do=get&target=pl_spacy_model_morfeusz-0.1.0.tar.gz' pl_spacy_model_morfeusz-0.1.0.tar.gz\n",
    "!python3 -m pip install pl_spacy_model_morfeusz-0.1.0.tar.gz\n",
    "\n",
    "# linkowanie modelu do spaCy\n",
    "!python3 -m spacy link pl_spacy_model_morfeusz pl_spacy_model_morfeusz -f\n",
    "\n",
    "# PO WYKONANIU NALEŻY ZRESETOWAĆ RUNTIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PtEk9Bt0N3u1"
   },
   "outputs": [],
   "source": [
    "### PYTHON 3\n",
    "import keras\n",
    "print(keras.__version__) # Powinno wyświetlić 2.3.1\n",
    "# ładowanie modelu\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pl_spacy_model_morfeusz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuPWYncPLyK4"
   },
   "source": [
    "# Część pierwsza - Tagowanie morfosyntaktyczne\n",
    "korzystamy z tagsetu NKJP\n",
    "Nasz tagger to słownikowy tagger Morfeusz2 + dezambiguacja za pomocą neuronowego Toyggera (LSTM)\n",
    "\n",
    "Każdy token t ma trzy interesujące nas atrybuty: \n",
    "- t.tag_ : klasa gramatyczna według polskiego tagsetu NKJP (http://nkjp.pl/poliqarp/help/ense2.html)\n",
    "- t.pos_ : klasa gramatyczna według międzynarodowego tagsetu UD (mapowana z NKJP)\n",
    "- t._.feats : customowy atrybut odpowiadający cechom morfosyntaktycznym (np. rodzajowi gramatycznemu, lub liczbie), poszczególne wartości cech są oddzielone dwukropkiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8MPXgjqU2LH"
   },
   "outputs": [],
   "source": [
    "txt = \"Nornica prowadzi zmierzchowo-nocny tryb życia, ale wychodzi również za dnia w poszukiwaniu pokarmu.\"\n",
    "doc = nlp(txt) # przetworzenie textu przez pipeline, na wyjściu dostajemy iterowalny obiekty klasy Doc, przechowujący tokeny\n",
    "for t in doc:\n",
    "  print(t, t.tag_, t.pos_, t._.feats) # wypisujemy interpretację morfosyntaktyczną każdego tokenu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZ2ioo6nW8eo"
   },
   "source": [
    "##Zadanie 1:\n",
    "Przetwórz tekst znajdujący się w zmiennej txt, oblicz proporcję czasowników (używając tagów UPOS), na podstawie tego oszacuj gatunek do którego należy tekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2eBs1u9VoMR"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "txt = requests.get(\"https://raw.githubusercontent.com/ryszardtuora/voicelab_resources/master/1.txt\").text\n",
    "\n",
    "# TODO\n",
    "\n",
    "doc = nlp(txt)\n",
    "nouns = 0\n",
    "verbs = 0\n",
    "for t in doc:\n",
    "  if t.pos_ == \"NOUN\":\n",
    "    nouns += 1\n",
    "  if t.pos_ == \"VERB\":\n",
    "    verbs += 1\n",
    "    \n",
    "print(verbs/nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2nv0aa1hcZr"
   },
   "source": [
    "##Zadanie 2:\n",
    "\n",
    "Oblicz proporcję rzeczowników w rodzaju żeńskim, do rzeczowników w rodzaju męskim, w tekście pod zmienną txt. Nie czyń założeń co do kolejności cech w t._.feats. Rodzaj męski w tagsecie NKJP jest rozbity na trzy \"podrodzaje\": m1, m2 i m3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MhHIUVJ4hxTf"
   },
   "outputs": [],
   "source": [
    "fem = \"lekarka\"\n",
    "print(nlp(fem)[0]._.feats) # sg:nom:f - liczba pojedyncza, mianownik, rodzaj żeński\n",
    "\n",
    "# TODO\n",
    "\n",
    "fems = 0\n",
    "males = 0\n",
    "for t in doc:\n",
    "  if t.pos_ == \"NOUN\":\n",
    "    if \"f\" in t._.feats.split(\":\"):\n",
    "      fems += 1\n",
    "    elif not \"n\" in t._.feats.split(\":\"):\n",
    "      males += 1\n",
    "\n",
    "print(fems/males)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "noq6HpCgY4q2"
   },
   "source": [
    "#Część druga - Lematyzacja i własności leksykalne\n",
    "Nasz model umożliwia słownikową lematyzację przy pomocy Morfeusza, do dezambiguacji (tutaj np. rozróżnienia między \"mamy\"-> \"mama\" i \"mamy\" -> \"mieć\" służy output taggera).\n",
    "\n",
    "Lematyzacja pozwala redukować redundancję informacyjną, i ułatwiać zadania takie jak streszczanie, i przeszukiwanie.\n",
    "\n",
    "Każdy z tokenów dodatkowo jest oznaczony ze względu na pewne własności leksykalne, np. :\n",
    "- is_stop - słowo należy do stoplisty (listy słów występujących najczęściej, a więc najmniej istotnych semantycznie)\n",
    "- is_oov - słowo znajduje się poza słownikiem, i.e. embeddingami wykorzystanymi w modelu\n",
    "- like_url - token ma strukturę url-a\n",
    "- like_num - token jest liczbą\n",
    "- is_alpha - token składa się tylko ze znaków alfabetycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFmVQkr2ZsKu"
   },
   "outputs": [],
   "source": [
    "txt = \"Zdenerwowany gen. Leese mówił przez telefon swym podwładnym walczącym pod Monte Cassino, że rozmawia z nimi ze schronu.\"\n",
    "doc = nlp(txt)\n",
    "for t in doc:\n",
    "  print(t.orth_, t.lemma_, t.is_oov, t.is_stop) # orth_ to atrybut odpowiadający formie słowa występującej w tekście"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2cBJlSpcziu"
   },
   "source": [
    "##Zadanie 3:\n",
    "\n",
    "Przetwórz tekst spod zmiennej txt, przekonwertuj go do listy lematów, usuń słowa ze stoplisty, oraz interpunkcję, wypisz dziesięć najczęściej pojawiających się lematów. Wypróbuj także opcję w której uwzględniamy tylko rzeczowniki\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HlxUtCEqcy3k"
   },
   "outputs": [],
   "source": [
    "txt = requests.get(\"https://raw.githubusercontent.com/ryszardtuora/voicelab_resources/master/2.txt\").text\n",
    "\n",
    "# TODO\n",
    "doc = nlp(txt)\n",
    "words = {}\n",
    "for t in doc:\n",
    "  if not (t.is_stop or t.tag_ == \"interp\"):\n",
    "    try:\n",
    "      words[t.lemma_] +=1\n",
    "    except KeyError:\n",
    "      words[t.lemma_] = 1\n",
    "listed = [(words[w],w) for w in words]\n",
    "srt = sorted(listed, reverse = True)\n",
    "print(srt[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yev0e8lQmwMm"
   },
   "source": [
    "#Część trzecia - parsowanie zależnościowe\n",
    "\n",
    "spaCy zawiera parser zależnościowy oparty o metodologię transition-based dependency parsing. \n",
    "Interesują nas tu cztery atrybuty:\n",
    " - t.head - link do tokenu będącego nadrzędnikiem tokenu t\n",
    " - t.dep_ - etykieta opisująca rodzaj zależności\n",
    " - t.subtree - generator opisujący poddrzewo rozpięte przez token t\n",
    " - t.ancestors - generator opisujący wszystkie przechodnie nadrzędniki tokenu t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7i9joa5n3ZK"
   },
   "outputs": [],
   "source": [
    "txt = \"Pierwsza wzmianka o Gdańsku pochodzi ze spisanego po łacinie w 999 Żywotu świętego Wojciecha.\"\n",
    "doc = nlp(txt)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "table = []\n",
    "for tok in doc:\n",
    "  tok_dic = {\"form\": tok.orth_, \"label\": tok.dep_, \"head\": t.head.orth_, \"subtree\": list(tok.subtree), \"dep_head\": list(tok.ancestors)}\n",
    "  table.append(tok_dic)\n",
    "df = pd.DataFrame(table)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6MI0U4Npeqn"
   },
   "source": [
    "### spaCy posiada wbudowaną wizualizację drzew zależnościowych\n",
    "colab nie pozwala na wyświetlanie etykiet zależności, ale wizualizacje przeprowadzane w innym środowisku będą je już zawierać."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFX-8z48peHV"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JS6zUzTKmN86"
   },
   "source": [
    "###Poniższa funkcja służy łatwej wizualizacji podstawowych własności tokenów z danego tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PDcn4tWTLBwz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def table(doc):\n",
    "  table = []\n",
    "  for tok in doc:\n",
    "    tok_dic = {\"form\": tok.orth_, \"lemma\": tok.lemma_, \"tag\": \":\".join([tok.tag_, tok._.feats]), \"dep_label\": tok.dep_, \"dep_head\": tok.head.orth_}\n",
    "    table.append(tok_dic)\n",
    "  return pd.DataFrame(table)\n",
    "\n",
    "txt = \"Wiadomość jest symboliczna, ale oznacza też początek długotrwałego trendu.\\\n",
    " Dochód na mieszkańca z uwzględnieniem realnej mocy nabywczej walut narodowych \\\n",
    " wyniósł w 2019 r. w Rzeczpospolitej Polskiej 33 891 dolarów, nieco więcej, niż w Portugalii \\\n",
    " (33 665 dolarów). Jednak Fundusz przewiduje, że w tym roku portugalska gospodarka\\\n",
    "  będzie się rozwijać w tempie 1,6 proc. wobec 3,1 proc. w przypadku gospodarki \\\n",
    "  polskiej. Nożyce między oboma krajami będą się więc rozwierać.\"\n",
    "\n",
    "doc = nlp(txt)\n",
    "\n",
    "tab = table(doc)\n",
    "\n",
    "print(tab.to_string()) # prosty hack na wypisanie całej tabeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_EwypobSuHQZ"
   },
   "source": [
    "###Czasami interesują nas większe całości niż pojedyncze tokeny, np. rzeczowniki często łączą się z przymiotnikami w frazy, żeby znajdować takie całości w tekście możemy korzystać z Matchera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rgH3Y2ybhYxp"
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "doc.is_tagged = True # tymczasowy hack, będzie zbędny w kolejnej wersji modelu\n",
    "pattern = [{\"TAG\": \"adj\"}, {\"TAG\": \"subst\"}]\n",
    "matcher.add(\"AdjSubst\", None, pattern) # nazwa, funkcja, wzór\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "  print(doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tev_A5DLp04E"
   },
   "source": [
    "#### Niemniej w języku polskim, ze względu na swobodę szyku, takie rozwiązanie jest mało owocne, przymiotnik nie musi zawsze poprzedzać rzeczownika który określa, tego typu zależności są widoczne dopiero na poziomie analizy gramatycznej\n",
    "\n",
    "### W gramatykach zależnościowych możliwa jest rekonstrukcja fraz rzeczownikowych (Noun Phrase - NP)\n",
    "Aby to zrobić, musimy wydobyć z tekstu wszystkie rzeczowniki, zebrać wszystkie rozpinane przez nie poddrzewa, i wyrzucić ze zbioru te poddrzewa, które są już podfrazą większej frazy rzeczownikowej. To frazy rzeczownikowe, a nie same rzeczowniki, są jednostkami które mają dobrze określone znaczenie. Np. w zdaniu:\n",
    "\n",
    "###*Żona Pawła jest blondynką.* \n",
    "\n",
    "nie mówimy o Pawle, ani o jakiejś abstrakcyjnej żonie, tylko o konkretnej kobiecie, którą wyodrębniamy przez fakt że jest żoną Pawła."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKlWTF1aqNgm"
   },
   "source": [
    "##Zadanie 4:\n",
    "\n",
    "Napisz funkcję która na wejściu pobiera dokument, a na wyjściu zwraca listę fraz rzeczownikowych zgodnie z podanym powyżej opisem. Następnie wyodrębnij wszystkie frazy rzeczownikowe z tekstu pod zmienną txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0WkRuafBp0F8"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "def get_nps(doc):\n",
    "  # Wyszukuje w sparsowanym dokumencie wszystkie maksymalne frazy rzeczownikowe\n",
    "  def is_contained(stree, stree_list):\n",
    "    # Sprawdza czy podana fraza jest podfrazą innej, większej frazy\n",
    "    return any([all([t in st for t in stree]) for st in stree_list])\n",
    "  \n",
    "  nouns = [t for t in doc if t.tag_ == \"subst\"] # to może być zależne od wersji niemorfeuszowej\n",
    "  subtrees = [list(n.subtree) for n in nouns]\n",
    "  to_eliminate = True\n",
    "  while to_eliminate:\n",
    "    # pętla do eliminowania fraz będących częścią większych fraz rzeczownikowych\n",
    "    to_remove = []\n",
    "    for st in subtrees:\n",
    "      if is_contained(st, [x for x in subtrees if x != st]):\n",
    "        to_remove.append(st)\n",
    "    if to_remove == []:\n",
    "      to_eliminate = False\n",
    "    else:\n",
    "      subtrees = [st for st in subtrees if st not in to_remove]\n",
    "  return subtrees\n",
    "\n",
    "for st in get_nps(nlp(txt)):\n",
    "  print(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YG0hKgyWwc4N"
   },
   "source": [
    "###Parser zależnościowy pozwala także na dzielenie dokumentów na zdania w sposób bardziej inteligentny, niż posługując się regułami interpunkcji.\n",
    "###Za zdanie uważamy nieprzerwaną sekwencję tokenów które są powiązane relacjami zależnościowymi.\n",
    "###Zdania są zapisane w atrybucie doc.sents dokumentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fH4rg0X4wwIB"
   },
   "outputs": [],
   "source": [
    "for s in doc.sents:\n",
    "  print(s)\n",
    "\n",
    "displacy.render(doc, jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DoLg35vSy_be"
   },
   "source": [
    "##Zadanie 5:\n",
    "\n",
    "Struktura gramatyczna zdania jest szczególnie istotna psychologicznie. Niektóre własności drzewa zależnościowego mogą sprawiać że zdanie jest trudne do zrozumienia. Do własności tych należy przede wszystkim nieprojektywność, ale także średnia długość zależności (liczona w tokenach .\n",
    "\n",
    "Napisz funkcję która liczy średnią długość krawędzi zależnościowej w drzewie. Załóżmy że krawędź łącząca słowa sąsiadujące ze sobą ma długość 1. Pamiętaj o tym, ile jest krawędzi w drzewie.\n",
    "\n",
    "Policz wartość tej statystyki dla podanych zdań."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DuRVCiSy9DR"
   },
   "outputs": [],
   "source": [
    "s0 = \"Jan kocha piękną i inteligentną Marię.\"\n",
    "s1 = \"Lingwistycznie zorientowani filozofowie twierdzą, że wiele problemów dotychczasowej filozofii to pseudoproblemy, wynikające z pojęć pozbawionych precyzyjnej definicji\"\n",
    "s2 = \"Zdaniem filozofów paradygmatu lingwistycznego, wiele problemów, którymi dotychczas zajmowała się filozofia było pseudoproblemami, wynikającymi z nieprecyzyjnego zdefiniowania używanych pojęć.\"\n",
    "displacy.render(nlp(s1), jupyter = True)\n",
    "\n",
    "\n",
    "# TODO\n",
    "\n",
    "def avg_dep_len(doc):\n",
    "  dlens = 0\n",
    "  for t in doc:\n",
    "    if t.dep_ != \"ROOT\":\n",
    "      dlens += abs(t.i - t.head.i)\n",
    "  return dlens / (len(doc) - 1)\n",
    "\n",
    "\n",
    "print(s0, avg_dep_len(nlp(s0)))\n",
    "print(s1, avg_dep_len(nlp(s1)))\n",
    "print(s2, avg_dep_len(nlp(s2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6d5JbrSsnhm"
   },
   "source": [
    "#Część czwarta - Rozpoznawanie jednostek nazewniczych (NER)\n",
    "####Nasz model do spaCy wykorzystuje 6 rodzajów etykiet:\n",
    "- placeName - miejsca antropogeniczne, np. Dania, Londyn\n",
    "- geogName - naturalne miejsca geograficzne, np. Tatry, Kreta\n",
    "- persName - imiona i nazwiska osób, np. J. F. Kennedy, gen. Maczek\n",
    "- orgName - nazwy organizacji, np. NATO, Unia Europejska\n",
    "- date - daty, np. 22 marca 1999, druga połowa kwietnia\n",
    "- time - czas, np. 18:55, pięć po dwunastej\n",
    "\n",
    "####Nie pozwala na wykrywanie zagnieżdżonych jednostek nazewniczych, np. [placeName: **aleja** [persName: **Piłsudskiego**]]\n",
    "\n",
    "####Wykryte wzmianki są przechowywane w atrybucie doc.ents dokumentu, każda z tych wzmianek ma atrybut ent.label_, w którym przechowywana jest jej etykieta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Bkf6kOWt8GK"
   },
   "outputs": [],
   "source": [
    "for e in doc.ents:\n",
    "  print(e, e.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zniWaxZxvYZz"
   },
   "source": [
    "###displaCy pozwala także na wizualizację jednostek nazewniczych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CJOleJmIvfGs"
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBxZzfaX28_M"
   },
   "source": [
    "##Zadanie 6:\n",
    "\n",
    "W zmiennej txt znajduje się dokument historyczny, wydobądź z niego wszystkie *zdania* zawierające daty, i po kolei je wypisz, rozważ możliwe sposoby automatycznego szeregowania dat wyrażonych w różny sposób (po ewentualnym sprowadzeniu ich do kanonicznej postaci). Rozwiązanie tego problemu jest trudne, i istnieją do niego odrębne narzędzia, umożliwiają one np. automatyczną rekonstrukcję chronologii wydarzeń."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "glLD6N0V3Uo2"
   },
   "outputs": [],
   "source": [
    "txt = requests.get(\"https://raw.githubusercontent.com/ryszardtuora/voicelab_resources/master/3.txt\").text\n",
    "\n",
    "# TODO\n",
    "\n",
    "doc = nlp(txt)\n",
    "events = []\n",
    "for s in doc.sents:\n",
    "  ents = s.ents\n",
    "  labs = [e.label_ for e in ents]\n",
    "  if \"date\" in labs:\n",
    "    events.append(s)\n",
    "\n",
    "for s in events:\n",
    "  print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4a5Izt4Tsae"
   },
   "source": [
    "# Część piąta - podobieństwo w języku\n",
    "Większość powyższych metod opiera się o embeddingi, czyli funkcje przypisujące słowom wektory w przestrzeni wielowymiarowej (najczęściej 100 lub 300-wymiarowej). Wektory te nie są przypisywane arbitralnie, lecz własności tej przestrzeni mają w jakiś sposób odzwierciedlać własności języka. Np. odległość między wektorami powinna odpowiadać odległości między słowami, którą możemy rozumieć jako podobieństwo znaczeń."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwvGXnaON7CB"
   },
   "source": [
    "###Bardzo popularny przykład \"arytmetyki słów\":\n",
    "znaczenie słów jest reprezentowane przez wektory, dla których mamy zdefiniowane operacje matematyczne. Możemy więc odjąć od znaczenia słowa \"królowa\", znaczenie słowa \"kobieta\", i dodać doń znaczenie słowa \"mężczyzna\", licząc iż wektor będący wynikiem takiego działania odpowiada słowu \"król\". W praktyce wektor taki najprawdopodobniej nie ma interpretacji, ale możemy znaleźć najbliższy wektor który ma jakąkolwiek interpretację przeszukując słownik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jb9ZXk4a8ety"
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    " \n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    " \n",
    "man = nlp.vocab['mężczyzna'].vector\n",
    "woman = nlp.vocab['kobieta'].vector\n",
    "queen = nlp.vocab['królowa'].vector\n",
    "king = nlp.vocab['król'].vector\n",
    " \n",
    "# We now need to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
    "maybe_king = man - woman + queen\n",
    "computed_similarities = []\n",
    " \n",
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors\n",
    "    if not word.has_vector:\n",
    "        continue\n",
    " \n",
    "    similarity = cosine_similarity(maybe_king, word.vector)\n",
    "    computed_similarities.append((word, similarity))\n",
    " \n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "print([w[0].text for w in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMw2hDepJ9EM"
   },
   "source": [
    "###spaCy umożliwia liczenie podobieństwa między słowami \n",
    "####jest ono proporcjonalne do cosinusa kąta wektorami je reprezentującymi. Odpowiednia funkcja jest zdefiniowana dla leksemów (tutaj oznaczają one słowa z pominięciem kontekstu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXJB6l6IIRJX"
   },
   "outputs": [],
   "source": [
    "w1 = \"pies\"\n",
    "w2 = \"psami\"\n",
    "w3 = \"zwierzę\"\n",
    "w4 = \"buldog\"\n",
    "w5 = \"policjant\"\n",
    "w6 = \"strażak\"\n",
    "w7 = \"marchewka\"\n",
    "w8 = \"słońce\"\n",
    "\n",
    "def similarity(w1, w2):\n",
    "  w1_lex = nlp.vocab[w1]\n",
    "  w2_lex = nlp.vocab[w2]\n",
    "  if w1_lex.has_vector and w2_lex.has_vector:\n",
    "    sim = w1_lex.similarity(w2_lex)\n",
    "    print(\"{} vs. {} -> {}\".format(w1, w2, sim))\n",
    "  else:\n",
    "    print(\"Jedno ze słów nie ma reprezentacji wektorowej.\")\n",
    "for w in [w2, w3, w4, w5, w6, w7, w8]:\n",
    "  similarity(w1, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SKbMLgI7GKd"
   },
   "source": [
    "## Zadanie 7:\n",
    "\n",
    "nlp.vocab może być traktowany jako iterator (stanowi słownik, choć nie wszystkim słowom przypisane są wektory). Napisz funkcję thesaurus(word) która dla podanego słowa znajduje dziesięć najbardziej podobnych słów. Pamiętaj o złożoności obliczeniowej sortowania! \n",
    "\n",
    "Możesz wykorzystać lematyzację, oraz tagowanie morfosyntaktyczne, by pozbyć się zbędnych słów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-i6ko6L67Fpa"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "def thesaurus(word):\n",
    "  synonyms = []\n",
    "  minimum = 0\n",
    "  w_lex = nlp.vocab[word]\n",
    "  for candidate in nlp.vocab:\n",
    "    if candidate.has_vector and candidate != w_lex:\n",
    "      similarity = w_lex.similarity(candidate)\n",
    "      if similarity > minimum:\n",
    "        synonyms.append((similarity, candidate.orth_))\n",
    "      if len(synonyms) >10:\n",
    "        synonyms = sorted(synonyms, reverse = True)\n",
    "        synonyms = synonyms[:-1]\n",
    "        minimum = synonyms[-1][0]\n",
    "\n",
    "  return synonyms\n",
    "\n",
    "syn = (thesaurus(\"pies\"))\n",
    "for x in syn:\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K29AazUEXUoD"
   },
   "source": [
    "Nic nie stoi na przeszkodzie by wykorzystywać model do przetwarzania dużych tekstów do których mamy dostęp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRi3nd8QTX6x"
   },
   "outputs": [],
   "source": [
    "!wget https://wolnelektury.pl/media/book/txt/homer-iliada.txt\n",
    "import re\n",
    "\n",
    "with open(\"homer-iliada.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "  iliada = f.read()\n",
    "\n",
    "#with open(\"ludzie-bezdomni-tom-pierwszy.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "#  iliada = f.read()\n",
    "\n",
    "\n",
    "# usuwanie podziału na wersy\n",
    "regex = re.compile(r\"([^\\n])\\n([^\\n])\")\n",
    "iliada = regex.sub(lambda m: m[1] + \" \" + m[2], iliada)\n",
    "# usuwanie zbędnych pustych linii\n",
    "regex = re.compile(r\"\\n{1,}\")\n",
    "iliada = regex.sub(\"\\n\", iliada)\n",
    "#print(iliada)\n",
    "\n",
    "## dzielenie na paragrafy po tabulatorach i znakach nowej linii\n",
    "splitregex = re.compile(r\"\\n| {5}\")\n",
    "paragraphs = [p for p in splitregex.split(iliada) if p != \"\"]\n",
    "\n",
    "num_pars = len(paragraphs)\n",
    "print(\"Liczba paragrafów: {}\".format(num_pars))\n",
    "\n",
    "docs = []\n",
    "milestones = [int(num_pars/10) * x for x in range(1,11)]\n",
    "for i, p in enumerate(paragraphs):\n",
    "  if i in milestones:\n",
    "    print(\"Przetworzono {}% paragrafów\".format(int((i / num_pars) * 100)))\n",
    "  docs.append(nlp(p)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSPKJujSW5x8"
   },
   "source": [
    "Większe jednostki językowe również można reprezentować wektorowo, i liczyć dla nich podobieństwa. Jest to jednak spore uproszczenie, które pomija całkowicie zależności składniowe.\n",
    "\n",
    "W tym przykładzie próbujemy znaleźć fragment Iliady który pasuje do podanego opisu wydarzeń."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qf5b67IwUafn"
   },
   "outputs": [],
   "source": [
    "wzorzec1 = nlp(\"Pryjam płacze nad martwym synem.\")\n",
    "wzorzec2 = nlp(\"Achilles daje Patroklosowi swój pancerz.\")\n",
    "\n",
    "def find_doc(schema):\n",
    "  maxsim = 0\n",
    "  for doc in docs:\n",
    "    if doc.similarity(wzorzec1) > maxsim:\n",
    "      maxsim = doc.similarity(wzorzec1)\n",
    "      maxdoc = doc\n",
    "  return (maxsim, maxdoc)\n",
    "\n",
    "print(wzorzec1, find_doc(wzorzec1))\n",
    "print(wzorzec2, find_doc(wzorzec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLTbvc_8a6cC"
   },
   "source": [
    "##Zadanie 8 (dodatkowe):\n",
    "\n",
    "Flexer to komponent służący do odmiany słów. Użytkownik na wejściu podaje token (i.e. słowo pochodzące z dokumentu, nie sam string!), oraz pożądaną charakterystykę morfosyntaktyczną. Napisz funkcję która wykrywa imię w wiadomości *msg* (nie wybieraj go po pozycji w zdaniu, lecz wykorzystaj NER), i konstruuje wypowiedź postaci \"Masz wiadomość od [imię]\" gdzie imię zostaje odmienione do dopełniacza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Isa2IsJ4a50Q"
   },
   "outputs": [],
   "source": [
    "flexer = nlp.get_pipe(\"flexer\")\n",
    "# pierwszym argumentem jest token, drugim argumentem jest pożądana charakterystyka morfosyntaktyczna\n",
    "# można zmienić kilka cech na raz, oddzielając je dwukropkiem, np.\n",
    "doc = nlp(\"Napadły na nas psy.\")\n",
    "psy = doc[-2]\n",
    "flexed = flexer.flex(psy, \"gen:sg\")\n",
    "print(flexed)\n",
    "\n",
    "\n",
    "msg = \"Paweł napisał do Ciebie wiadomość.\"\n",
    "# TODO\n",
    "doc = nlp(msg)\n",
    "blueprint = \"Masz wiadomość od {}\"\n",
    "name = [e for e in doc.ents if e.label_ == \"persName\"][0][0]\n",
    "gen = flexer.flex(name, \"gen\")\n",
    "print(blueprint.format(gen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "WuCXiOBFcXZE",
    "outputId": "d3c70e56-3d56-45dd-ad9f-4331ff70f595"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'psa'"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Gdańsk_ans.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
